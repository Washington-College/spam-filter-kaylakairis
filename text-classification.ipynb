{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Spam Filter, based off some work from AIDevNepal's AI Saturday Workshop #8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "It is one of the simplest machine learning model for text classification. It uses the probabilistic distribution of tokens/words (counts) to classify documents. It is based on infamous **Bayes Theorem,**\n",
    "\n",
    "$$P(B \\mid A) = \\frac{P(A \\mid B) P(B)}{P(A)}$$\n",
    "\n",
    "## Text Classification\n",
    "Say we have document $D$ that belongs to class $C$ (spam or ham, for example). So, using Bayes' Theorem we can infer:  \n",
    "\n",
    "$$P(C \\mid D) = \\frac{P(D \\mid C) P(C)}{P(D)}$$\n",
    "\n",
    "We know a document is made up of tokens (combination of tokens, commonly referred to as **ngram language model**):  \n",
    "$$D = [d_1, d_2, d_3, ...].$$\n",
    "So,\n",
    "$$\n",
    "P(C | D)\n",
    "= \\frac{P(d_1 | C) P(d_2 | C) P(d_3 | C) ... P(C)}{P(D)}\n",
    "$$\n",
    "\n",
    "Remember, we have seggregated $P(D | C)$ to individual probabilities of individual tokens (ngrams) making up the document $D$. This is why Naive Bayes classifier is **Naive** - it assumes  each tokens are independent of each other.  \n",
    "  \n",
    "Think it of as two independent events $A$ and $B$. What's the probability of both events occuring simultaneously?  \n",
    "$$P(A \\text{ and } B) = P(A) P(B)$$\n",
    "\n",
    "Now we can infer the Probabities $P(d_i | C)$ as :  \n",
    "` (count(di) that belongs to class C) / (total number of tokens).` This is saying that $P(d_i \\mid C)$ is a multinomial distribution.\n",
    "\n",
    "### Putting Things Into Perspective\n",
    "And that is how we can find the probability of document $D$ belonging to class $C$ assuming independence of individual features(ngrams). \n",
    "\n",
    "Now, say we have classes\n",
    "$C_1, C_2, C_3, ...,$\n",
    "And we want to classify a test document $D$. All we have to do is find the probabilty of this document $D$\n",
    "beloning to each of the classes. And we choose the class where $P(D \\mid C)$ is the highest.\n",
    "\n",
    "#### Training Steps\n",
    "It's nothing but counting the \"stuff\" that matter.\n",
    "- tokenize the documents for each classes (words, or pairs of words, etc)\n",
    "- each token can be unigram, [bigram](https://en.wikipedia.org/wiki/Bigram)\n",
    "- extract features for each token -> counts\n",
    "\n",
    "#### Let's classify\n",
    "- extract features (count) for the document to be classified\n",
    "- calculate $P(C_1 | D)$\n",
    "- Calculate $P(C_2 | D)$\n",
    "- Calculate $P(C_3 | D)$\n",
    "- choose the Class $C_i$ that has max probability\n",
    "\n",
    "**Side note**:  \n",
    "Since $P(D)$ is constant, we can ignore the denominator part and just focus on the numerator's products.  \n",
    "\n",
    "So, all we are doing is choosing class $C_i$ according to $\\argmax{ P(C_i | D) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noob documents for training :P\n",
    "spam = [\n",
    "    \"you have won a lottery\",\n",
    "    \"congratulations! you have a bonus\",\n",
    "    \"this is bomb\",\n",
    "    \"to use the credit, please click the link\",\n",
    "    \"thank you for subscription. please click the link\",\n",
    "    \"bomb\"\n",
    "]\n",
    "Y_spam = [1 for i in range(len(spam)) ]\n",
    "\n",
    "non_spam = [\n",
    "    \"i am awesome\",\n",
    "    \"i have a meeting tomorrow\",\n",
    "    \"you are smart\",\n",
    "    \"get me out of here\",\n",
    "    \"call me later\"\n",
    "]\n",
    "Y_non_spam = [0 for i in range(len(non_spam)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i am awesome', 'i have a meeting tomorrow', 'you are smart', 'get me out of here', 'call me later']\n"
     ]
    }
   ],
   "source": [
    "print(non_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y_non_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you have won a lottery',\n",
       " 'congratulations! you have a bonus',\n",
       " 'this is bomb',\n",
       " 'to use the credit, please click the link',\n",
       " 'thank you for subscription. please click the link',\n",
       " 'bomb',\n",
       " 'i am awesome',\n",
       " 'i have a meeting tomorrow',\n",
       " 'you are smart',\n",
       " 'get me out of here',\n",
       " 'call me later']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam+non_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2)).fit(spam + non_spam)\n",
    "X_train_vectorized = count_vectorizer.transform(spam + non_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['am', 'am awesome', 'are', 'are smart', 'awesome', 'bomb', 'bonus',\n",
       "       'call', 'call me', 'click', 'click the', 'congratulations',\n",
       "       'congratulations you', 'credit', 'credit please', 'for',\n",
       "       'for subscription', 'get', 'get me', 'have', 'have bonus',\n",
       "       'have meeting', 'have won', 'here', 'is', 'is bomb', 'later',\n",
       "       'link', 'lottery', 'me', 'me later', 'me out', 'meeting',\n",
       "       'meeting tomorrow', 'of', 'of here', 'out', 'out of', 'please',\n",
       "       'please click', 'smart', 'subscription', 'subscription please',\n",
       "       'thank', 'thank you', 'the', 'the credit', 'the link', 'this',\n",
       "       'this is', 'to', 'to use', 'tomorrow', 'use', 'use the', 'won',\n",
       "       'won lottery', 'you', 'you are', 'you for', 'you have'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 1 1 0 0 0 0 0 2 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1\n",
      "  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes Model\n",
    "model = MultinomialNB(alpha=0.1)\n",
    "model.fit(X_train_vectorized, Y_spam + Y_non_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "documents = [\n",
    "    \"call you\",\n",
    "    \"you have won\"\n",
    "]\n",
    "predictions = model.predict(count_vectorizer.transform(documents))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# convert to pandas dataframe for seamless training\n",
    "spam_df = pd.DataFrame(spam, columns=['text'])\n",
    "spam_df['target'] = 1\n",
    "non_spam_df = pd.DataFrame(non_spam, columns=['text'])\n",
    "non_spam_df['target'] = 0\n",
    "\n",
    "# final data\n",
    "data = pd.concat([spam_df, non_spam_df], ignore_index=True)\n",
    "data\n",
    "\n",
    "# feature extraction\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2)).fit(data['text'])\n",
    "X_train_vectorized = count_vectorizer.transform(data['text'])\n",
    "model = MultinomialNB(alpha=0.1)\n",
    "model.fit(X_train_vectorized, Y_spam + Y_non_spam)\n",
    "documents = [\n",
    "    \"call you\",\n",
    "    \"you have won\"\n",
    "]\n",
    "predictions = model.predict(count_vectorizer.transform(documents))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do real training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "target",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9b800b90-bba7-4321-8770-d4521ba4e9f3",
       "rows": [
        [
         "0",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...",
         "ham"
        ],
        [
         "1",
         "Ok lar... Joking wif u oni...",
         "ham"
        ],
        [
         "2",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's",
         "spam"
        ],
        [
         "3",
         "U dun say so early hor... U c already then say...",
         "ham"
        ],
        [
         "4",
         "Nah I don't think he goes to usf, he lives around here though",
         "ham"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text target\n",
       "0  Go until jurong point, crazy.. Available only ...    ham\n",
       "1                      Ok lar... Joking wif u oni...    ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   spam\n",
       "3  U dun say so early hor... U c already then say...    ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...    ham"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load training set\n",
    "data = pd.read_csv('data/spam.csv')\n",
    "data.head()\n",
    "# the csv has two columns, text and target, where target is spam or ham. Convert the target to 1 or 0,\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = np.where(data['target']=='spam',1, 0)\n",
    "print(len(data))\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use scikit-learn's train_test_split to split the data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = # your code here. Use test_size=0.2 and random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform a CountVectorizer to the training data as we did in the example above.\n",
    "count_vectorizer = #your code here.\n",
    "X_train_vectorized = #your code here.\n",
    "X_train_vectorized.toarray().shape # should be (4457, 42915)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Multinomial Naive Bayes model as we did above. Use alpha=0.1, but also try other values\n",
    "model = #your code here\n",
    "#fit the model\n",
    "#your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the space below, [read about different ways to score classifiers](https://scikit-learn.org/stable/api/sklearn.metrics.html). Then, score your model using at least 4 different metrics, one of which should be a confusion matrix. At the top, I've loaded in some scores, but there are others you can use. Read about each score, and interpret your score in the context of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions as we did above. Then, compare the predictions to the actual values in Y_test using at least 4 different metrics, one of which should be a confusion matrix.\n",
    "predictions = #your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, have some fun and imagine your own legit and fraudulent emails, and see how the classifier you built does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs = [\n",
    "    \"you have won a lottery\",\n",
    "    \"click the link\",\n",
    "    \"Hi Rashid, how about lunch at the dining hall tomorrow to talk this over?\",\n",
    "]\n",
    "predictions = model.predict(vectorizer.transform(test_docs))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write some concluding thoughts to your future self. What have you learned in doing this activity? Why does Naive Bayes' with multinomial distributions work well in this setting? Why aren't we using a more complicated algorithm? What are the pros and cons of the approach we took?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
